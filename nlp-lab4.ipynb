{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport random\nfrom bs4 import BeautifulSoup\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport torch\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:26:16.714090Z","iopub.execute_input":"2025-08-10T06:26:16.714395Z","iopub.status.idle":"2025-08-10T06:26:24.979882Z","shell.execute_reply.started":"2025-08-10T06:26:16.714372Z","shell.execute_reply":"2025-08-10T06:26:24.979069Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n2025-08-10 06:26:21.779838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754807181.797786     970 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754807181.803165     970 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install --upgrade transformers accelerate datasets evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv',encoding=\"ISO-8859-1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:24:03.858331Z","iopub.execute_input":"2025-08-10T06:24:03.859006Z","iopub.status.idle":"2025-08-10T06:24:04.483087Z","shell.execute_reply.started":"2025-08-10T06:24:03.858977Z","shell.execute_reply":"2025-08-10T06:24:04.482312Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Remove HTML tags\ndef remove_html(text):\n    return BeautifulSoup(text, \"html.parser\").get_text()\n\n# Remove special characters & digits\ndef remove_special_chars(text):\n    return re.sub(r'[^a-zA-Z\\s]', '', text)\n\n# Lowercase text\ndef to_lowercase(text):\n    return text.lower()\n\n# Remove stopwords (optional for transformer models, more important for classical ML)\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    words = text.split()\n    return ' '.join([word for word in words if word not in stop_words])\n\n# Apply cleaning\ndf['review'] = df['review'].apply(remove_html)\ndf['review'] = df['review'].apply(to_lowercase)\ndf['review'] = df['review'].apply(remove_special_chars)\n# Uncomment below if you want stopword removal\ndf['review'] = df['review'].apply(remove_stopwords)\n\n# Map sentiment to numeric\ndf['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Shuffle dataset\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:26:29.805683Z","iopub.execute_input":"2025-08-10T06:26:29.806570Z","iopub.status.idle":"2025-08-10T06:26:32.850800Z","shell.execute_reply.started":"2025-08-10T06:26:29.806545Z","shell.execute_reply":"2025-08-10T06:26:32.850044Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df['review']\ny = df['label']\nimport pandas as pd\n\ntrain_df, test_df = train_test_split(df.drop('sentiment',axis=1), test_size=0.2, random_state=42, stratify=df[\"label\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:26:54.247228Z","iopub.execute_input":"2025-08-10T06:26:54.247986Z","iopub.status.idle":"2025-08-10T06:26:54.303338Z","shell.execute_reply.started":"2025-08-10T06:26:54.247951Z","shell.execute_reply":"2025-08-10T06:26:54.302659Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:26:57.610130Z","iopub.execute_input":"2025-08-10T06:26:57.610432Z","iopub.status.idle":"2025-08-10T06:26:57.977496Z","shell.execute_reply.started":"2025-08-10T06:26:57.610412Z","shell.execute_reply":"2025-08-10T06:26:57.976935Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\n\n# We won't fix model_name here, so tokenization can be redone quickly per model\ndef tokenize_data(tokenizer, train_dataset, test_dataset, max_length=256):\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"review\"],  # column name from CSV\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length\n        )\n\n    train_tok = train_dataset.map(tokenize_fn, batched=True)\n    test_tok = test_dataset.map(tokenize_fn, batched=True)\n\n    # Keep only the required columns\n    train_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n    test_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n    return train_tok, test_tok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:27:01.695969Z","iopub.execute_input":"2025-08-10T06:27:01.696447Z","iopub.status.idle":"2025-08-10T06:27:01.701379Z","shell.execute_reply.started":"2025-08-10T06:27:01.696424Z","shell.execute_reply":"2025-08-10T06:27:01.700673Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ndef compute_f1(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")  # weighted handles imbalance\n    return {\"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:27:04.426655Z","iopub.execute_input":"2025-08-10T06:27:04.427376Z","iopub.status.idle":"2025-08-10T06:27:04.431644Z","shell.execute_reply.started":"2025-08-10T06:27:04.427350Z","shell.execute_reply":"2025-08-10T06:27:04.430737Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\ndef finetune_model(model_name, train_dataset, test_dataset, epochs=2):\n    print(f\"\\n--- Training {model_name} ---\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\n    # Tokenize datasets for this model\n    train_tok, test_tok = tokenize_data(tokenizer, train_dataset, test_dataset)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results_{model_name}\",\n        save_strategy=\"no\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=epochs,\n        weight_decay=0.01,\n        logging_dir=f\"./logs_{model_name}\",\n        report_to=\"none\",\n        no_cuda=not torch.cuda.is_available()\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_tok,\n        eval_dataset=test_tok,\n        tokenizer=tokenizer,\n        compute_metrics=compute_f1\n    )\n\n    trainer.train()\n    metrics = trainer.evaluate()\n    return metrics[\"eval_f1\"], model, tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:27:07.121066Z","iopub.execute_input":"2025-08-10T06:27:07.121490Z","iopub.status.idle":"2025-08-10T06:27:07.129120Z","shell.execute_reply.started":"2025-08-10T06:27:07.121460Z","shell.execute_reply":"2025-08-10T06:27:07.128139Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"models_to_try = [\n    \"bert-base-uncased\"\n    \"roberta-base\",\n    \"google/electra-base-discriminator\",\n    \"microsoft/deberta-base\",\n    \"distilbert-base-uncased\"\n]\n\nresults = {}\ntrained_models = {}\n\n# Small subset for quick comparison\ntrain_subset = train_dataset.shuffle(seed=42).select(range(5000))\ntest_subset = test_dataset.shuffle(seed=42).select(range(2000))\n\nfor model_name in models_to_try:\n    f1, model, tokenizer = finetune_model(model_name, train_subset, test_subset, epochs=2)\n    results[model_name] = f1\n    trained_models[model_name] = (model, tokenizer)\n\nprint(\"\\nModel performance on subset:\")\nfor name, score in results.items():\n    print(f\"{name}: F1 = {score:.4f}\")\n\nbest_model_name = max(results, key=results.get)\nprint(f\"\\nBest model: {best_model_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:27:17.352231Z","iopub.execute_input":"2025-08-10T06:27:17.352941Z","iopub.status.idle":"2025-08-10T06:50:41.873414Z","shell.execute_reply.started":"2025-08-10T06:27:17.352915Z","shell.execute_reply":"2025-08-10T06:50:41.872839Z"}},"outputs":[{"name":"stdout","text":"\n--- Training bert-base-uncased ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94c8ee244e054b27b9bc5655021a3c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45e07797f3f4138afba6251bcf8521d"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_970/2593346365.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [314/314 04:11, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n--- Training roberta-base ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f384c698db8a47b7ae8db48390af9010"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"330f41f811a442d58a0a11d6fd68e0c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec76a2163244554a97092c2bd96c0ed"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_970/2593346365.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [314/314 04:26, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:16]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n--- Training google/electra-base-discriminator ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6397f1f3c9a94c68939e27467a1b2aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d004e064d1024ec09abde75b5b18d01e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cd4621f009148ee914624aa17d23c6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f45b19f4c948b6b34b42b79848c02c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8dc15ca5ec246e6a722c34f15618ec6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c2d27126964627afa7fb5efe3b6e3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a2225b7a124293831af1b2881f74ae"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_970/2593346365.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b03da147079644329bf1271e92b5d249"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [314/314 04:26, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n--- Training microsoft/deberta-base ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d8190611ac9401eabf7e6e878573bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ceaee694e24763b03b4be17013d3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080ca374198c4357931a1e3271c4236a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefca4ff088f46e7bdfb7bd3acc3245a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b6074bd1674aa186dd755dd9a4fb16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37d577ffa53145a4a2dbe7e294b78175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2f19f14d8fa4e759ecbdf33265b3ef7"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_970/2593346365.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b66cfa8d724403ca8d8890c3904ab9e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [314/314 05:58, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n--- Training distilbert-base-uncased ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583b3cf6aebe4540823712e3411aaad0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23830a0468c44a7ab165283e21be558b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351b26fb6acc49e8aa07df3f470a322c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8162fce70a604c6faf354b0ee83ec219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4772627f732a42e787270fec708e0a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21821f14e24e4491828318d25ee9a017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09ee4f4f445489a880a0f1c4b0398cc"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_970/2593346365.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [314/314 02:13, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nModel performance on subset:\nbert-base-uncased: F1 = 0.8710\nroberta-base: F1 = 0.8939\ngoogle/electra-base-discriminator: F1 = 0.9125\nmicrosoft/deberta-base: F1 = 0.8980\ndistilbert-base-uncased: F1 = 0.8740\n\nBest model: google/electra-base-discriminator\n","output_type":"stream"}],"execution_count":15}]}