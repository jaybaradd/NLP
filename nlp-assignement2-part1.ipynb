{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814},{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:41:55.143045Z","iopub.execute_input":"2025-09-07T17:41:55.143359Z","iopub.status.idle":"2025-09-07T17:42:01.308752Z","shell.execute_reply.started":"2025-09-07T17:41:55.143337Z","shell.execute_reply":"2025-09-07T17:42:01.307892Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load IMDB dataset from Kaggle input\ndf = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Encode sentiment: positive → 1, negative → 0\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Train-test split\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    df['review'].values, df['sentiment'].values, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:01.310082Z","iopub.execute_input":"2025-09-07T17:42:01.310463Z","iopub.status.idle":"2025-09-07T17:42:02.953790Z","shell.execute_reply.started":"2025-09-07T17:42:01.310442Z","shell.execute_reply":"2025-09-07T17:42:02.953236Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def tokenize(text):\n    return re.findall(r\"\\b\\w+\\b\", text.lower())\n\n# Tokenize \ntrain_tokens = [tokenize(text) for text in train_texts]\ntest_tokens = [tokenize(text) for text in test_texts]\n\n# Build vocabulary\nall_tokens = [token for sublist in train_tokens for token in sublist]\ncounter = Counter(all_tokens)\nvocab_size = 20000  # top 20,000 words\n\nmost_common = counter.most_common(vocab_size - 2)\nword2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\nfor idx, (word, _) in enumerate(most_common, start=2):\n    word2idx[word] = idx\nidx2word = {idx: word for word, idx in word2idx.items()}\n\n# Encode text\ndef encode(tokens):\n    return [word2idx.get(token, word2idx[\"<UNK>\"]) for token in tokens]\n\nX_train_encoded = [torch.tensor(encode(tokens)) for tokens in train_tokens]\nX_test_encoded = [torch.tensor(encode(tokens)) for tokens in test_tokens]\n\n# Pad sequences\nX_train_padded = pad_sequence(X_train_encoded, batch_first=True, padding_value=0)\nX_test_padded = pad_sequence(X_test_encoded, batch_first=True, padding_value=0)\ny_train_tensor = torch.tensor(train_labels)\ny_test_tensor = torch.tensor(test_labels)\n\nprint(f\"Vocabulary size: {len(word2idx)}\")\nprint(f\"Padded train shape: {X_train_padded.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:02.954674Z","iopub.execute_input":"2025-09-07T17:42:02.954981Z","iopub.status.idle":"2025-09-07T17:42:13.300241Z","shell.execute_reply.started":"2025-09-07T17:42:02.954957Z","shell.execute_reply":"2025-09-07T17:42:13.299314Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 20000\nPadded train shape: torch.Size([40000, 2525])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class IMDBDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_dataset = IMDBDataset(X_train_padded, y_train_tensor)\ntest_dataset = IMDBDataset(X_test_padded, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:13.302287Z","iopub.execute_input":"2025-09-07T17:42:13.302619Z","iopub.status.idle":"2025-09-07T17:42:13.309822Z","shell.execute_reply.started":"2025-09-07T17:42:13.302599Z","shell.execute_reply":"2025-09-07T17:42:13.309107Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_glove_embeddings(glove_path, word2idx, embedding_dim=100):\n    embeddings_index = {}\n    with open(glove_path, encoding='utf-8') as f:\n        for line in f:\n            values = line.strip().split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = vector\n\n    matrix_len = len(word2idx)\n    weights_matrix = np.zeros((matrix_len, embedding_dim))\n\n    for word, i in word2idx.items():\n        weights_matrix[i] = embeddings_index.get(word, np.random.normal(scale=0.6, size=(embedding_dim,)))\n    \n    return torch.tensor(weights_matrix, dtype=torch.float32)\n\n# Use GloVe from Kaggle input\nglove_path = \"/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\"\nembedding_dim = 100\nglove_weights = load_glove_embeddings(glove_path, word2idx, embedding_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:13.310680Z","iopub.execute_input":"2025-09-07T17:42:13.310943Z","iopub.status.idle":"2025-09-07T17:42:23.044568Z","shell.execute_reply.started":"2025-09-07T17:42:13.310925Z","shell.execute_reply":"2025-09-07T17:42:23.043952Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class RNNModel(nn.Module):\n    def __init__(self, embedding_weights):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n        self.rnn = nn.RNN(embedding_weights.shape[1], 128, batch_first=True)\n        self.fc = nn.Linear(128, 1)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, h_n = self.rnn(embedded)\n        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n\nclass LSTMModel(nn.Module):\n    def __init__(self, embedding_weights):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n        self.lstm = nn.LSTM(embedding_weights.shape[1], 128, batch_first=True)\n        self.fc = nn.Linear(128, 1)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (h_n, _) = self.lstm(embedded)\n        return torch.sigmoid(self.fc(h_n[-1]))\n\nclass RNNLearned(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=100):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, 128, batch_first=True)\n        self.fc = nn.Linear(128, 1)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, h_n = self.rnn(embedded)\n        return torch.sigmoid(self.fc(h_n.squeeze(0)))\n\nclass LSTMLearned(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=100):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, 128, batch_first=True)\n        self.fc = nn.Linear(128, 1)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, (h_n, _) = self.lstm(embedded)\n        return torch.sigmoid(self.fc(h_n[-1]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:23.050957Z","iopub.execute_input":"2025-09-07T17:42:23.051283Z","iopub.status.idle":"2025-09-07T17:42:23.060943Z","shell.execute_reply.started":"2025-09-07T17:42:23.051255Z","shell.execute_reply":"2025-09-07T17:42:23.060175Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=5):\n    model.to(device)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for x_batch, y_batch in train_loader:\n            x_batch, y_batch = x_batch.to(device), y_batch.float().to(device)\n            optimizer.zero_grad()\n            output = model(x_batch).squeeze()\n            loss = criterion(output, y_batch)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n    \n    evaluate_model(model, val_loader)\n\ndef evaluate_model(model, data_loader):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(device)\n            output = model(x_batch).squeeze().cpu().numpy() > 0.5\n            preds.extend(output)\n            labels.extend(y_batch.numpy())\n    acc = accuracy_score(labels, preds)\n    print(f\"Accuracy: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:23.061887Z","iopub.execute_input":"2025-09-07T17:42:23.062210Z","iopub.status.idle":"2025-09-07T17:42:23.080041Z","shell.execute_reply.started":"2025-09-07T17:42:23.062184Z","shell.execute_reply":"2025-09-07T17:42:23.079301Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\" RNN + GloVe\")\nmodel_rnn_glove = RNNModel(glove_weights)\ntrain_model(model_rnn_glove, train_loader, test_loader)\n\nprint(\"\\n LSTM + GloVe\")\nmodel_lstm_glove = LSTMModel(glove_weights)\ntrain_model(model_lstm_glove, train_loader, test_loader)\n\nprint(\"\\n RNN + Learned Embedding\")\nmodel_rnn_learned = RNNLearned(len(word2idx))\ntrain_model(model_rnn_learned, train_loader, test_loader)\n\nprint(\"\\n LSTM + Learned Embedding\")\nmodel_lstm_learned = LSTMLearned(len(word2idx))\ntrain_model(model_lstm_learned, train_loader, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:42:23.080866Z","iopub.execute_input":"2025-09-07T17:42:23.081165Z","iopub.status.idle":"2025-09-07T17:52:24.394715Z","shell.execute_reply.started":"2025-09-07T17:42:23.081141Z","shell.execute_reply":"2025-09-07T17:52:24.393917Z"}},"outputs":[{"name":"stdout","text":" RNN + GloVe\nEpoch 1, Loss: 0.6949\nEpoch 2, Loss: 0.6957\nEpoch 3, Loss: 0.6952\nEpoch 4, Loss: 0.6952\nEpoch 5, Loss: 0.6948\nAccuracy: 0.4996\n\n LSTM + GloVe\nEpoch 1, Loss: 0.6935\nEpoch 2, Loss: 0.6933\nEpoch 3, Loss: 0.6932\nEpoch 4, Loss: 0.6932\nEpoch 5, Loss: 0.6932\nAccuracy: 0.5039\n\n RNN + Learned Embedding\nEpoch 1, Loss: 0.6960\nEpoch 2, Loss: 0.6942\nEpoch 3, Loss: 0.6938\nEpoch 4, Loss: 0.6938\nEpoch 5, Loss: 0.6942\nAccuracy: 0.4961\n\n LSTM + Learned Embedding\nEpoch 1, Loss: 0.6937\nEpoch 2, Loss: 0.6932\nEpoch 3, Loss: 0.6932\nEpoch 4, Loss: 0.6932\nEpoch 5, Loss: 0.6932\nAccuracy: 0.4960\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}